# -----------------------------------------------------------------------------
# DATASET AND THETA SCALER
# -----------------------------------------------------------------------------

dataset:
  file_path: "$FM4AR_DATASETS_DIR/FM_dataset_zenith.hdf"
  n_train_samples: 1_000_000
  n_valid_samples: 100_000
  random_seed: 42

theta_scaler:
  method: "identity"

# -----------------------------------------------------------------------------
# MODEL ARCHITECTURE
# -----------------------------------------------------------------------------

model:

  # General settings
  model_type: "fmpe"
  random_seed: 42
  t_theta_with_glu: True
  context_with_glu: False
  sigma_min: 0.0001
  time_prior_exponent: 2.0

  # Embedding network for context
  context_embedding_net:
    - block_type: "Concatenate"
      kwargs:
        keys: ["flux"]

  # Embedding network for (t, theta)
  t_theta_embedding_net:
    - block_type: "DenseResidualNet"
      kwargs:
        hidden_dims: [16]
        activation: "GELU"
        output_dim: 16
        dropout: 0.0
        use_batch_norm: True

  # Vector field network
  vectorfield_net:
    network_type: "DenseResidualNet"
    kwargs:
      hidden_dims:
        - 256
        - 256
        - 512
        - 512
        - 1024
        - 512
        - 512
        - 256
        - 256
        - 128
        - 128
        - 64
        - 64
        - 32
        - 16

      activation: "GELU"
      dropout: 0.05
      use_layer_norm: true

# -----------------------------------------------------------------------------
# TRAINING
# -----------------------------------------------------------------------------

training:

  stage_0:
    backup_interval: 10
    batch_size: 5_000
    data_transforms:
      - type: "Subsample"
        kwargs:
          factor: 1
    early_stopping:
      stage_patience: 30
    epochs: 250
    float32_matmul_precision: "high"
    gradient_clipping:
      enabled: False
      max_norm: 1.0
    logprob_evaluation:
      interval: 10
      n_samples: 2048
      ode_solver:
        method: "dopri5"
        tolerance: 1.0e-4
    optimizer:
      type: "AdamW"
      kwargs:
        lr: 1.0e-03
    scheduler:
      type: "ReduceLROnPlateau"
      kwargs:
        factor: 0.5
        patience: 6
        min_lr: 1.0e-8
      # type: "CosineAnnealingLR"
      # kwargs:
      #   T_max: 300
    use_amp: False  # only works on GPU

# -----------------------------------------------------------------------------
# LOCAL SETTINGS
# -----------------------------------------------------------------------------

local:

  # Device ("cpu" or "cuda"; or "auto")
  device: "cuda:0"

  # Maximum runtime (in seconds) per cluster job
  max_runtime: 28_800

  # Settings for HTCondor
  # htcondor:
  #   bid: 50
  #   n_cpus: 8
  #   n_gpus: 1
  #   memory_cpus: 200_000
  #   gpu_type: "H100"

  # # Settings for Weights & Biases, remove if not used
  wandb:
    project: "fm4na"
